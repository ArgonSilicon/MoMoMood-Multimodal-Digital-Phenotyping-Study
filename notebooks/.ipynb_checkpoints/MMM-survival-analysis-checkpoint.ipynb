{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MoMo-Mood PHQ9 survival analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces survival analysis conducted for MoMo-Mood study, using PHQ-9 scores as proxy for participants adherance to study.\n",
    "The notebook is structured as follows:\n",
    "1. Kapplan-Meier estimator curves for exploratory analysis and visualizations\n",
    "2. Log-Rank tests for participant group\n",
    "\n",
    "The analysis assumes that\n",
    "\n",
    "Survival analysis is conducted using [lifelines](https://lifelines.readthedocs.io/en/latest/) Python library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from lifelines.statistics import multivariate_logrank_test\n",
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path().resolve().parent))  # go up one folder\n",
    "#from settings import DATA_DIR, OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msettings\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImported settings.py from:\u001b[39m\u001b[38;5;124m\"\u001b[39m, settings\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttributes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mdir\u001b[39m(settings))\n",
      "File \u001b[0;32m/scratch/cs/networks-nima-mmm2018/Arsi/MoMoMood_survival_analysis/settings.py:24\u001b[0m\n\u001b[1;32m     20\u001b[0m     cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Path(cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cfg\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[1;32m     25\u001b[0m cfg \u001b[38;5;241m=\u001b[39m load_config()\n\u001b[1;32m     26\u001b[0m DATA_DIR \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "import settings\n",
    "print(\"Imported settings.py from:\", settings.__file__)\n",
    "print(\"Attributes:\", dir(settings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(settings))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phq_data = pd.read_csv(DATA_DIR, index_col=0)\n",
    "phq_data = phq_data[[\"group\", \"user\", \"idx\", \"PHQ9\"]]\n",
    "# Filter out rows where PHQ9 is NaN\n",
    "filtered_df = phq_data.dropna(subset=[\"PHQ9\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data availability visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'group' and 'idx' to get the count of unique users at each timepoint\n",
    "user_counts = filtered_df.groupby(['group', 'idx'])['user'].nunique().reset_index(name='user_count')\n",
    "\n",
    "# Get the initial user count for each group\n",
    "initial_user_counts = filtered_df.groupby('group')['user'].nunique().reset_index(name='initial_user_count')\n",
    "\n",
    "# Merge the initial user counts with the user counts at each timepoint\n",
    "merged = pd.merge(user_counts, initial_user_counts, on='group')\n",
    "\n",
    "# Calculate the percentage of users still providing data at each timepoint\n",
    "merged['percentage_remaining'] = (merged['user_count'] / merged['initial_user_count']) * 100\n",
    "\n",
    "# Pivot the table to get the desired format\n",
    "res_piv = merged.pivot(index='idx', columns='group', values='percentage_remaining').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'group' and 'idx' to get the count of unique users at each timepoint\n",
    "user_counts = df.groupby(['group', 'idx'])['user'].nunique().reset_index(name='user_count')\n",
    "\n",
    "# Get the initial user count for each group\n",
    "initial_user_counts = df.groupby('group')['user'].nunique().reset_index(name='initial_user_count')\n",
    "\n",
    "# Merge the initial user counts with the user counts at each timepoint\n",
    "merged = pd.merge(user_counts, initial_user_counts, on='group')\n",
    "\n",
    "# Calculate the percentage of users still providing data at each timepoint\n",
    "merged['percentage_remaining'] = (merged['user_count'] / merged['initial_user_count']) * 100\n",
    "\n",
    "# Pivot the table to get the desired format\n",
    "result = merged.pivot(index='idx', columns='group', values='percentage_remaining').fillna(0)\n",
    "\n",
    "# Calculate total initial user count\n",
    "total_initial_users = df['user'].nunique()\n",
    "\n",
    "# Calculate the total user count at each timepoint across all groups\n",
    "total_users_at_each_timepoint = df.groupby('idx')['user'].nunique()\n",
    "\n",
    "# Calculate the overall percentage of users remaining at each timepoint\n",
    "total_percentage_remaining = (total_users_at_each_timepoint / total_initial_users) * 100\n",
    "\n",
    "# Add the total percentage remaining as a new column to the result DataFrame\n",
    "result['total'] = total_percentage_remaining\n",
    "\n",
    "# Display the result\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"User Data Retention Table with Total\", dataframe=result)\n",
    "\n",
    "# Optionally, print the DataFrame to check\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set style to remove grid and set the background to white\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Replace inf and -inf with NaN in the dataframe\n",
    "result.replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Set font sizes\n",
    "plt.rc('font', size=16)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=20)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=18)     # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=18)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=18)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=18)    # legend fontsize\n",
    "plt.rc('figure', titlesize=20)   # fontsize of the figure title\n",
    "\n",
    "# Use Seaborn lineplot with different line styles\n",
    "sns.lineplot(data=result, x=result.index, y='bd', label='BD', linestyle='-.', linewidth=1)\n",
    "sns.lineplot(data=result, x=result.index, y='bpd', label='BPD', linestyle='--', linewidth=1)\n",
    "sns.lineplot(data=result, x=result.index, y='control', label='Control', linestyle=':', linewidth=1)\n",
    "sns.lineplot(data=result, x=result.index, y='mdd', label='MDD', linestyle=(0, (5, 2)), linewidth=1)\n",
    "sns.lineplot(data=result, x=result.index, y='total', label='Average', linestyle='-', color='black', linewidth=1, alpha=0.7)\n",
    "\n",
    "# Removing axis spines\n",
    "sns.despine(top=True, right=True)\n",
    "\n",
    "# Adding a vertical line at index 4\n",
    "plt.axvline(x=4, color='purple', linestyle=':', linewidth=2, label='8 weeks')\n",
    "\n",
    "# Removing grid\n",
    "plt.grid(False)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Time index (14-day period)', fontsize=20)\n",
    "plt.ylabel('Adherence (%)', fontsize=20)\n",
    "plt.title('Study adherence over time by group',fontsize=16)\n",
    "\n",
    "# Adding legend with larger title font\n",
    "legend = plt.legend(title='Group')\n",
    "plt.setp(legend.get_title(), fontsize=20)  # Set font size of the legend title\n",
    "\n",
    "# Add extra bottom and left margins\n",
    "plt.subplots_adjust(left=0.18, bottom=0.18)\n",
    "\n",
    "# Save the plot as an image\n",
    "plt.savefig(\"PHQ9_adherence.png\", dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_piv = merged.pivot(index='idx', columns='group', values='percentage_remaining').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where PHQ9 is NaN\n",
    "filtered_df = phq_data.dropna(subset=[\"PHQ9\"])\n",
    "\n",
    "# Group by 'user' and find the maximum 'idx' for each user\n",
    "max_idx_per_user = filtered_df.groupby(\"user\")[\"idx\"].max()\n",
    "\n",
    "# max_idx_per_user now contains the maximum index for each user\n",
    "\n",
    "# Merge with the original DataFrame to get the 'group' column\n",
    "merged_df = pd.merge(max_idx_per_user, phq_data, on=[\"user\", \"idx\"], how=\"left\")\n",
    "\n",
    "# Drop duplicates to retain only the unique combination for each user\n",
    "final_df = merged_df.drop_duplicates(subset=[\"user\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'event_occurred' based on the 'idx' column\n",
    "final_df[\"event_occurred\"] = final_df[\"idx\"].apply(lambda x: 1 if x < 26 else 0)\n",
    "final_df.rename(columns={\"idx\": \"time_to_event\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named 'final_df'\n",
    "# final_df.columns = ['user', 'time_to_event', 'group', 'event_occurred']\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results = pd.DataFrame()\n",
    "\n",
    "# Get the unique time points and create a complete range\n",
    "time_points = final_df['time_to_event'].unique()\n",
    "time_points.sort()\n",
    "complete_time_points = pd.RangeIndex(start=time_points.min(), stop=time_points.max() + 1, step=1)\n",
    "\n",
    "# Get the unique groups\n",
    "groups = final_df['group'].unique()\n",
    "\n",
    "# Initialize a dictionary to hold the survival percentages\n",
    "survival_dict = {group: [] for group in groups}\n",
    "total_survival = []\n",
    "\n",
    "# Calculate the percentage of remaining users for each group at each time point\n",
    "for time in complete_time_points:\n",
    "    total_users_all_groups = 0\n",
    "    total_remaining_all_groups = 0\n",
    "    for group in groups:\n",
    "        total_users = final_df[final_df['group'] == group]['user'].nunique()\n",
    "        remaining_users = final_df[(final_df['group'] == group) & (final_df['time_to_event'] >= time)]['user'].nunique()\n",
    "        survival_percentage = (remaining_users / total_users) * 100\n",
    "        survival_dict[group].append(survival_percentage)\n",
    "        total_users_all_groups += total_users\n",
    "        total_remaining_all_groups += remaining_users\n",
    "    \n",
    "    # Calculate the pooled survival percentage across all groups\n",
    "    pooled_survival_percentage = (total_remaining_all_groups / total_users_all_groups) * 100\n",
    "    total_survival.append(pooled_survival_percentage)\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "survival_df = pd.DataFrame(survival_dict, index=complete_time_points)\n",
    "\n",
    "# Add the pooled survival percentage column\n",
    "survival_df['total_survival_percentage'] = total_survival\n",
    "\n",
    "# Fill missing values using linear interpolation for each column\n",
    "survival_df = survival_df.interpolate(method='linear')\n",
    "\n",
    "# Set the index name\n",
    "survival_df.index.name = 'time_to_event'\n",
    "\n",
    "# Save to CSV (if needed)\n",
    "csv_file_path = './survival_analysis.csv'\n",
    "survival_df.to_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, you can print the DataFrame to ch\n",
    "survival_df.head().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named 'final_df'\n",
    "# final_df.columns = ['user', 'time_to_event', 'group', 'event_occurred']\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results = pd.DataFrame()\n",
    "\n",
    "# Get the unique time points and create a complete range\n",
    "time_points = final_df['time_to_event'].unique()\n",
    "time_points.sort()\n",
    "complete_time_points = pd.RangeIndex(start=time_points.min(), stop=time_points.max() + 1, step=1)\n",
    "\n",
    "# Get the unique groups\n",
    "groups = final_df['group'].unique()\n",
    "\n",
    "# Initialize a dictionary to hold the survival percentages\n",
    "survival_dict = {group: [] for group in groups}\n",
    "\n",
    "# Calculate the percentage of remaining users for each group at each time point\n",
    "for group in groups:\n",
    "    group_survival = []\n",
    "    for time in complete_time_points:\n",
    "        total_users = final_df[final_df['group'] == group]['user'].nunique()\n",
    "        remaining_users = final_df[(final_df['group'] == group) & (final_df['time_to_event'] >= time)]['user'].nunique()\n",
    "        survival_percentage = (remaining_users / total_users) * 100\n",
    "        group_survival.append(survival_percentage)\n",
    "    survival_dict[group] = group_survival\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "survival_df = pd.DataFrame(survival_dict, index=complete_time_points)\n",
    "\n",
    "# Fill missing values using forward fill for each column\n",
    "survival_df = survival_df.ffill()\n",
    "\n",
    "# Set the index name\n",
    "survival_df.index.name = 'time_to_event'\n",
    "\n",
    "# Save to CSV (if needed)\n",
    "csv_file_path = './survival_analysis.csv'\n",
    "survival_df.to_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, you can print the DataFrame to check\n",
    "survival_df.head(5).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index name\n",
    "#survival_df.set_index('time_to_event', inplace=True)\n",
    "survival_df = result\n",
    "# Format the DataFrame to one decimal place\n",
    "survival_df = survival_df.applymap(lambda x: f\"{x:.1f}\")\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = './survival_analysis.csv'\n",
    "survival_df.to_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_df.head().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "csv_file_path = './survival_analysis.csv'\n",
    "survival_df.to_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Kaplan-Meier estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "# Set font sizes for the plot (comparable with previous plots)\n",
    "plt.rc('font', size=16)\n",
    "plt.rc('axes', titlesize=20)\n",
    "plt.rc('axes', labelsize=18)\n",
    "plt.rc('xtick', labelsize=16)\n",
    "plt.rc('ytick', labelsize=16)\n",
    "plt.rc('legend', fontsize=16)\n",
    "plt.rc('figure', titlesize=20)\n",
    "\n",
    "# Plotting setup\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Group renaming (mapping CONTROL -> Control)\n",
    "group_mapping = {\"CONTROL\": \"Control\"}\n",
    "\n",
    "# Iterate over each group in alphabetical order\n",
    "for group in sorted(final_df[\"group\"].unique()):  \n",
    "    group_df = final_df[final_df[\"group\"] == group]\n",
    "    group_name = group_mapping.get(group, group)\n",
    "\n",
    "    # Fit the model\n",
    "    kmf.fit(\n",
    "        durations=group_df[\"time_to_event\"],\n",
    "        event_observed=group_df[\"event_occurred\"],\n",
    "        label=str(group_name).capitalize(),\n",
    "    )\n",
    "\n",
    "    # Plot survival function (no shading)\n",
    "    kmf.plot_survival_function(ax=ax, ci_show=False)\n",
    "\n",
    "# Keep only left & bottom spines (like your other plots)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(left=True, bottom=True)\n",
    "\n",
    "# Remove grid\n",
    "ax.grid(False)\n",
    "\n",
    "# Labels\n",
    "plt.xlabel(\"Time index (14-day period)\", fontsize=20)\n",
    "plt.ylabel(\"Survival probability\", fontsize=20)\n",
    "\n",
    "# Legend with title\n",
    "legend = ax.legend(title='Group')\n",
    "plt.setp(legend.get_title(), fontsize=20)\n",
    "\n",
    "# Adjust margins\n",
    "plt.subplots_adjust(left=0.18, bottom=0.18)\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('PHQ9_survival.png', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Log-rank test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the log-rank test\n",
    "results = multivariate_logrank_test(\n",
    "    event_durations=final_df[\"time_to_event\"],\n",
    "    event_observed=final_df[\"event_occurred\"],\n",
    "    groups=final_df[\"group\"],\n",
    ")\n",
    "\n",
    "# Print the test results\n",
    "print(results.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_pairs = list(itertools.combinations(final_df[\"group\"].unique(), 2))\n",
    "comparison_results = []\n",
    "\n",
    "for group1, group2 in group_pairs:\n",
    "    data_group1 = final_df[final_df[\"group\"] == group1]\n",
    "    data_group2 = final_df[final_df[\"group\"] == group2]\n",
    "\n",
    "    result = logrank_test(\n",
    "        data_group1[\"time_to_event\"],\n",
    "        data_group2[\"time_to_event\"],\n",
    "        event_observed_A=data_group1[\"event_occurred\"],\n",
    "        event_observed_B=data_group2[\"event_occurred\"],\n",
    "    )\n",
    "\n",
    "    comparison_results.append(\n",
    "        {\"group_1\": group1, \"group_2\": group2, \"p-val\": result.p_value}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the original p-values\n",
    "p_values = [result[\"p-val\"] for result in comparison_results]\n",
    "\n",
    "# Apply Benjamini-Hochberg FDR correction\n",
    "p_adjusted = multipletests(p_values, alpha=0.05, method=\"fdr_bh\")[1]\n",
    "\n",
    "# Update the list with adjusted p-values\n",
    "for result, adj_p in zip(comparison_results, p_adjusted):\n",
    "    result[\"adjusted p-val\"] = adj_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df = pd.DataFrame(comparison_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "N_MMM",
   "language": "python",
   "name": "new_mmm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
